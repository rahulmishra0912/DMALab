{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: Prepare a learning model to make predictions.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite \n",
    "### Note: Install xgboostclassifier on your laptop or lab machine by typing following command:\n",
    "\n",
    "   $ conda install py-xgboost\n",
    "\n",
    "   Test in the cell of ipynb by typing and running:  import xgboost as xgb  \n",
    "   If this doesn't throw error it works fine (warnings can be omitted)\n",
    "   \n",
    "   xgboost (https://xgboost.readthedocs.io/en/latest/)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Lab 01 and 02 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Reading data...completed\n",
      "Fixing timestamps...\n",
      "Fixing timestamps...completed\n",
      "Droped date_first_booking column...\n",
      "Fixing age column...\n",
      "Fixing age column...completed\n",
      "Filling first_affiliate_tracked column...\n",
      "Filling first_affiliate_tracked column...completed\n",
      "One Hot Encoding categorical data...\n",
      "One Hot Encoding categorical data...completed\n",
      "Adding new fields...\n",
      "Adding new fields...completed\n",
      "Droping fields...\n",
      "Droping fields...completed\n",
      "Reading sessions data...\n",
      "Reading sessions data...completed\n",
      "Determing primary device...\n",
      "Determing primary device...completed\n",
      "Determing secondary device...\n",
      "Determing secondary device...completed\n",
      "Aggregating actions taken...\n",
      "Converting action column...\n",
      "Converting action_type column...\n",
      "Converting action_detail column...\n",
      "Combining results...\n",
      "Combining results...completed\n"
     ]
    }
   ],
   "source": [
    "#Loading the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Reading data...\")\n",
    "train_file = \"./traveler/train_users_2.csv\"\n",
    "df_train = pd.read_csv(train_file, header = 0,index_col=None)\n",
    "\n",
    "test_file = \"./traveler/test_users.csv\"\n",
    "df_test = pd.read_csv(test_file, header = 0,index_col=None)\n",
    "\n",
    "# Combining into one dataset for cleaning\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "print(\"Reading data...completed\")\n",
    "# Fixing date formats in Pandas - to_datetime\n",
    "## Change dates to specific format\n",
    "print(\"Fixing timestamps...\")\n",
    "df_all['date_account_created'] = pd.to_datetime(df_all['date_account_created'], format='%Y-%m-%d')\n",
    "df_all['timestamp_first_active'] = pd.to_datetime(df_all['timestamp_first_active'], format='%Y%m%d%H%M%S')\n",
    "print(\"Fixing timestamps...completed\")\n",
    "# Removing date_first_booking column\n",
    "df_all.drop('date_first_booking', axis = 1, inplace = True)\n",
    "print(\"Droped date_first_booking column...\")\n",
    "\n",
    "## Remove outliers function - [1]\n",
    "def remove_outliers(df, column, min_val, max_val):\n",
    "    col_values = df[column].values\n",
    "    df[column] = np.where(np.logical_or(col_values<=min_val, col_values>=max_val), np.NaN, col_values)\n",
    "    return df\n",
    "\n",
    "## Fixing age column - [2]\n",
    "print(\"Fixing age column...\")\n",
    "df_all = remove_outliers(df = df_all, column = 'age', min_val = 15, max_val = 90)\n",
    "df_all['age'].fillna(-1, inplace = True)\n",
    "print(\"Fixing age column...completed\")\n",
    "# Fill first_affiliate_tracked column\n",
    "print(\"Filling first_affiliate_tracked column...\")\n",
    "df_all['first_affiliate_tracked'].fillna(-1, inplace=True)\n",
    "print(\"Filling first_affiliate_tracked column...completed\")\n",
    "\n",
    "def convert_to_binary(df, column_to_convert):\n",
    "    categories = list(df[column_to_convert].drop_duplicates())\n",
    "\n",
    "    for category in categories:\n",
    "        cat_name = str(category).replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\").replace(\"-\", \"\").lower()\n",
    "        col_name = column_to_convert[:5] + '_' + cat_name[:10]\n",
    "        df[col_name] = 0\n",
    "        df.loc[(df[column_to_convert] == category), col_name] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "# One Hot Encoding\n",
    "print(\"One Hot Encoding categorical data...\")\n",
    "columns_to_convert = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser']\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    df_all = convert_to_binary(df=df_all, column_to_convert=column)\n",
    "    df_all.drop(column, axis=1, inplace=True)\n",
    "print(\"One Hot Encoding categorical data...completed\")\n",
    "\n",
    "# Add new date related fields\n",
    "print(\"Adding new fields...\")\n",
    "df_all['day_account_created'] = df_all['date_account_created'].dt.weekday\n",
    "df_all['month_account_created'] = df_all['date_account_created'].dt.month\n",
    "df_all['quarter_account_created'] = df_all['date_account_created'].dt.quarter\n",
    "df_all['year_account_created'] = df_all['date_account_created'].dt.year\n",
    "df_all['hour_first_active'] = df_all['timestamp_first_active'].dt.hour\n",
    "df_all['day_first_active'] = df_all['timestamp_first_active'].dt.weekday\n",
    "df_all['month_first_active'] = df_all['timestamp_first_active'].dt.month\n",
    "df_all['quarter_first_active'] = df_all['timestamp_first_active'].dt.quarter\n",
    "df_all['year_first_active'] = df_all['timestamp_first_active'].dt.year\n",
    "df_all['created_less_active'] = (df_all['date_account_created'] - df_all['timestamp_first_active']).dt.days\n",
    "print(\"Adding new fields...completed\")\n",
    "\n",
    "\n",
    "# Drop unnecessary columns\n",
    "print(\"Droping fields...\")\n",
    "columns_to_drop = ['date_account_created', 'timestamp_first_active', 'date_first_booking', 'country_destination']\n",
    "for column in columns_to_drop:\n",
    "    if column in df_all.columns:\n",
    "        df_all.drop(column, axis=1, inplace=True)\n",
    "print(\"Droping fields...completed\")\n",
    "\n",
    "## Loading sessions.csv data\n",
    "print(\"Reading sessions data...\")\n",
    "sessions_file = \"./traveler/sessions.csv\"\n",
    "df_sessions = pd.read_csv(sessions_file, header = 0,index_col=False)\n",
    "print(\"Reading sessions data...completed\")\n",
    "\n",
    "# Determine primary device\n",
    "print(\"Determing primary device...\")\n",
    "sessions_device = df_sessions.loc[:, ['user_id', 'device_type', 'secs_elapsed']]\n",
    "aggregated_lvl1 = sessions_device.groupby(['user_id', 'device_type'], as_index=False, sort=False).aggregate(np.sum)\n",
    "#aggregated_lvl1.head(10)\n",
    "idx = aggregated_lvl1.groupby(['user_id'], sort=False)['secs_elapsed'].transform(max) == aggregated_lvl1['secs_elapsed']\n",
    "#idx.head(10)\n",
    "df_sessions_primary = pd.DataFrame(aggregated_lvl1.loc[idx , ['user_id', 'device_type', 'secs_elapsed']])\n",
    "#df_sessions_primary.head(10)\n",
    "df_sessions_primary.rename(columns = {'device_type':'primary_device', 'secs_elapsed':'primary_secs'}, inplace=True)\n",
    "#df_sessions_primary.head(10)\n",
    "# Call user defined One Hot Encoding function\n",
    "df_sessions_primary = convert_to_binary(df=df_sessions_primary, column_to_convert='primary_device')\n",
    "#df_sessions_primary.head()\n",
    "df_sessions_primary.drop('primary_device', axis=1, inplace=True)\n",
    "#df_sessions_primary.head()\n",
    "print(\"Determing primary device...completed\")\n",
    "\n",
    "# Determine Secondary device\n",
    "print(\"Determing secondary device...\")\n",
    "remaining = aggregated_lvl1.drop(aggregated_lvl1.index[idx])\n",
    "remaining.head()\n",
    "idx = remaining.groupby(['user_id'], sort=False)['secs_elapsed'].transform(max) == remaining['secs_elapsed']\n",
    "df_sessions_secondary = pd.DataFrame(remaining.loc[idx , ['user_id', 'device_type', 'secs_elapsed']])\n",
    "df_sessions_secondary.rename(columns = {'device_type':'secondary_device', 'secs_elapsed':'secondary_secs'}, inplace=True)\n",
    "df_sessions_secondary = convert_to_binary(df=df_sessions_secondary, column_to_convert='secondary_device')\n",
    "df_sessions_secondary.drop('secondary_device', axis=1, inplace=True)\n",
    "print(\"Determing secondary device...completed\")\n",
    "\n",
    "# Count occurrences of value in a column\n",
    "def convert_to_counts(df, id_col, column_to_convert):\n",
    "    id_list = df[id_col].drop_duplicates()\n",
    "\n",
    "    df_counts = df.loc[:,[id_col, column_to_convert]]\n",
    "    df_counts['count'] = 1\n",
    "    df_counts = df_counts.groupby(by=[id_col, column_to_convert], as_index=False, sort=False).sum()\n",
    "\n",
    "    new_df = df_counts.pivot(index=id_col, columns=column_to_convert, values='count')\n",
    "    new_df = new_df.fillna(0)\n",
    "\n",
    "# Rename Columns\n",
    "    categories = list(df[column_to_convert].drop_duplicates())\n",
    "    for category in categories:\n",
    "        cat_name = str(category).replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\").replace(\"-\", \"\").lower()\n",
    "        col_name = column_to_convert + '_' + cat_name\n",
    "        new_df.rename(columns = {category:col_name}, inplace=True)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "# Aggregate and combine actions taken columns\n",
    "print(\"Aggregating actions taken...\")\n",
    "session_actions = df_sessions.loc[:,['user_id', 'action', 'action_type', 'action_detail']]\n",
    "columns_to_convert = ['action', 'action_type', 'action_detail']\n",
    "session_actions = session_actions.fillna('not provided')\n",
    "first = True\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    print(\"Converting \" + column + \" column...\")\n",
    "    current_data = convert_to_counts(df=session_actions, id_col='user_id', column_to_convert=column)\n",
    "\n",
    "# If first loop, current data becomes existing data, otherwise merge existing and current\n",
    "    if first:\n",
    "        first = False\n",
    "        actions_data = current_data\n",
    "    else:\n",
    "        actions_data = pd.concat([actions_data, current_data], axis=1, join='inner')\n",
    "\n",
    "\n",
    "# [4.1] Merge device datasets\n",
    "print(\"Combining results...\")\n",
    "df_sessions_primary.set_index('user_id', inplace=True)\n",
    "df_sessions_secondary.set_index('user_id', inplace=True)\n",
    "device_data = pd.concat([df_sessions_primary, df_sessions_secondary], axis=1, join=\"outer\")\n",
    "\n",
    "# [4.2] Merge device and actions datasets\n",
    "combined_results = pd.concat([device_data, actions_data], axis=1, join='outer')\n",
    "df_sessions_complete = combined_results.fillna(0)\n",
    "\n",
    "# [4.3] Merge user and session datasets\n",
    "df_all.set_index('id', inplace=True)\n",
    "df_all = pd.concat([df_all, df_sessions_complete], axis=1, join='inner')\n",
    "print(\"Combining results...completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gende_unknown</th>\n",
       "      <th>gende_male</th>\n",
       "      <th>gende_female</th>\n",
       "      <th>gende_other</th>\n",
       "      <th>signu_facebook</th>\n",
       "      <th>signu_basic</th>\n",
       "      <th>signu_google</th>\n",
       "      <th>signu_weibo</th>\n",
       "      <th>signu_0</th>\n",
       "      <th>...</th>\n",
       "      <th>action_detail_view_resolutions</th>\n",
       "      <th>action_detail_view_search_results</th>\n",
       "      <th>action_detail_view_security_checks</th>\n",
       "      <th>action_detail_view_user_real_names</th>\n",
       "      <th>action_detail_wishlist</th>\n",
       "      <th>action_detail_wishlist_content_update</th>\n",
       "      <th>action_detail_wishlist_note</th>\n",
       "      <th>action_detail_your_listings</th>\n",
       "      <th>action_detail_your_reservations</th>\n",
       "      <th>action_detail_your_trips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00023iyk9l</th>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0010k6l0om</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001wyh0pz8</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0028jgx1x1</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>002qnbzfs5</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 720 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age  gende_unknown  gende_male  gende_female  gende_other  \\\n",
       "00023iyk9l   31              1           0             0            0   \n",
       "0010k6l0om   -1              1           0             0            0   \n",
       "001wyh0pz8   -1              1           0             0            0   \n",
       "0028jgx1x1   -1              1           0             0            0   \n",
       "002qnbzfs5   26              0           0             1            0   \n",
       "\n",
       "            signu_facebook  signu_basic  signu_google  signu_weibo  signu_0  \\\n",
       "00023iyk9l               0            1             0            0        1   \n",
       "0010k6l0om               0            1             0            0        1   \n",
       "001wyh0pz8               0            1             0            0        0   \n",
       "0028jgx1x1               0            1             0            0        1   \n",
       "002qnbzfs5               1            0             0            0        0   \n",
       "\n",
       "                      ...             action_detail_view_resolutions  \\\n",
       "00023iyk9l            ...                                          0   \n",
       "0010k6l0om            ...                                          0   \n",
       "001wyh0pz8            ...                                          0   \n",
       "0028jgx1x1            ...                                          0   \n",
       "002qnbzfs5            ...                                          0   \n",
       "\n",
       "            action_detail_view_search_results  \\\n",
       "00023iyk9l                                  5   \n",
       "0010k6l0om                                 10   \n",
       "001wyh0pz8                                 66   \n",
       "0028jgx1x1                                  9   \n",
       "002qnbzfs5                                125   \n",
       "\n",
       "            action_detail_view_security_checks  \\\n",
       "00023iyk9l                                   0   \n",
       "0010k6l0om                                   0   \n",
       "001wyh0pz8                                   0   \n",
       "0028jgx1x1                                   0   \n",
       "002qnbzfs5                                   0   \n",
       "\n",
       "            action_detail_view_user_real_names  action_detail_wishlist  \\\n",
       "00023iyk9l                                   0                       0   \n",
       "0010k6l0om                                   0                       0   \n",
       "001wyh0pz8                                   0                       0   \n",
       "0028jgx1x1                                   0                       0   \n",
       "002qnbzfs5                                   0                       0   \n",
       "\n",
       "            action_detail_wishlist_content_update  \\\n",
       "00023iyk9l                                      4   \n",
       "0010k6l0om                                      8   \n",
       "001wyh0pz8                                      0   \n",
       "0028jgx1x1                                      0   \n",
       "002qnbzfs5                                      0   \n",
       "\n",
       "            action_detail_wishlist_note  action_detail_your_listings  \\\n",
       "00023iyk9l                            0                            0   \n",
       "0010k6l0om                            0                            0   \n",
       "001wyh0pz8                            0                            0   \n",
       "0028jgx1x1                            0                            0   \n",
       "002qnbzfs5                            0                            0   \n",
       "\n",
       "            action_detail_your_reservations  action_detail_your_trips  \n",
       "00023iyk9l                                0                         2  \n",
       "0010k6l0om                                0                         0  \n",
       "001wyh0pz8                                0                         0  \n",
       "0028jgx1x1                                0                         0  \n",
       "002qnbzfs5                                0                         0  \n",
       "\n",
       "[5 rows x 720 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train1 = df_train\n",
    "df_test1 = df_test\n",
    "df_all1 = df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'id', u'date_account_created', u'timestamp_first_active',\n",
       "       u'date_first_booking', u'gender', u'age', u'signup_method',\n",
       "       u'signup_flow', u'language', u'affiliate_channel',\n",
       "       u'affiliate_provider', u'first_affiliate_tracked', u'signup_app',\n",
       "       u'first_device_type', u'first_browser', u'country_destination'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      NDF\n",
       "1      NDF\n",
       "2       US\n",
       "3    other\n",
       "4       US\n",
       "Name: country_destination, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['country_destination'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'id', u'date_account_created', u'timestamp_first_active',\n",
       "       u'date_first_booking', u'gender', u'age', u'signup_method',\n",
       "       u'signup_flow', u'language', u'affiliate_channel',\n",
       "       u'affiliate_provider', u'first_affiliate_tracked', u'signup_app',\n",
       "       u'first_device_type', u'first_browser'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'age', u'gende_unknown', u'gende_male', u'gende_female',\n",
       "       u'gende_other', u'signu_facebook', u'signu_basic', u'signu_google',\n",
       "       u'signu_weibo', u'signu_0',\n",
       "       ...\n",
       "       u'action_detail_view_resolutions', u'action_detail_view_search_results',\n",
       "       u'action_detail_view_security_checks',\n",
       "       u'action_detail_view_user_real_names', u'action_detail_wishlist',\n",
       "       u'action_detail_wishlist_content_update',\n",
       "       u'action_detail_wishlist_note', u'action_detail_your_listings',\n",
       "       u'action_detail_your_reservations', u'action_detail_your_trips'],\n",
       "      dtype='object', length=720)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone: Creating a learning model\n",
    "\n",
    "##### To predict the first booking destination country for each user based on the dataset created in earlier lab sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing an Algorithm (most crusual)\n",
    "\n",
    "### [1] Decision Tree - refer Applied Statistics notes\n",
    "Biggest problem: model overfitting\n",
    "\n",
    "Solution parameters setting: Stop model splitting once the records at a given node gets too small (minimum split) and when a certain number of splits have occurred (maximum depth).\n",
    "\n",
    "The problem is \n",
    "- how do you know how large you should grow the tree? \n",
    "- how do you set the parameters to avoid overfitting but still have an accurate model? \n",
    "\n",
    "Reality is that it is extremely difficult to know how to set the parameters. \n",
    "- Set them too conservatively and the model will lose too much predictive power. \n",
    "- Set them too aggressively and the model will start overfitting the data.\n",
    "\n",
    "BEST PART - methods have been found to reduce the risk of overfitting and increase predictive power of decisions trees, to train multiple trees (random forest, boosting)\n",
    "\n",
    "[1] The 'random forest' algorithm constructs a large number of different trees by randomly selecting the features that can be used to build each tree (as opposed to using all the features for each tree).\n",
    "\n",
    "[2] 'Boosting' algorithm which builds trees iteratively such that each tree learns from earlier trees. We focus on very popular 'XGBoost' algorithm.\n",
    "\n",
    "### [2] Alternative Models\n",
    "#### [2.1] K-Nearest Neighbors\n",
    "Classifies a given object by looking at the classification of the k most similar records and seeing how those records are classified. Also known as lazy learner.\n",
    "#### [2.2] Neural Networks\n",
    "Typicaly consists of three layers: an input layer, a hidden layer (although there can be multiple hidden layers) and an output layer.\n",
    "#### [2.3] Support Vector Machines\n",
    "Classifier which separates classes using kernel trick.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach for creating the model using XGBoost algorithm\n",
    "#### [1.1] k-fold Cross Validation\n",
    "##### Why? \n",
    "- one of the key risks when creating models is the risk of overfitting.\n",
    "- to guard against overfitting is to estimate the accuracy of the models on data that was not used to train the model i.e., using cross-validation method (different CV methods - https://www.cs.cmu.edu/~schneide/tut5/node42.html)\n",
    "\n",
    "##### How?\n",
    "<img src=\"./images/cross-validation.png\" height=\"400\" width=\"500\"/>\n",
    "#### [1.2] Parameter Tuning\n",
    "##### Why? \n",
    "- Parameter options: How many trees to build? How deep should each tree be? How much extra weight will be attached to each misclassified record? \n",
    "- Tuning these parameters to get the best results from the model is often one of the most time consuming things that data scientists do.\n",
    "\n",
    "##### How?\n",
    "- However, the process can be automated.\n",
    "\n",
    "### Even better, using the 'Scikit-Learn' package, \n",
    "- merge the parameter tuning and cross validation steps into one, allowing to search for the best combination of parameters while using k-fold cross validation to verify the results.\n",
    "\n",
    "\n",
    "#### [1.3] Training the Model\n",
    "   ##### First, define training dataset and split the training data into the three main components – \n",
    "  (i) the user IDs (we don’t want to use these for training as they are randomly generated),\n",
    "  \n",
    "  (ii) the features to use for training (X), and \n",
    "  \n",
    "  (iii) the categories we are trying to predict (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xgboost as xgb\n",
    "\n",
    "#from xgboost.sklearn import XGBClassifier\n",
    "#from sklearn import cross_validation, decomposition, grid_search\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare training data for modelling\n",
    "df_train1.set_index('id', inplace=True)\n",
    "df_train1 = pd.concat([df_train1['country_destination'], \n",
    "                       df_all1], axis=1, join='inner')\n",
    "#df_train2 = df_train1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73815, 721)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_destination</th>\n",
       "      <th>age</th>\n",
       "      <th>gende_unknown</th>\n",
       "      <th>gende_male</th>\n",
       "      <th>gende_female</th>\n",
       "      <th>gende_other</th>\n",
       "      <th>signu_facebook</th>\n",
       "      <th>signu_basic</th>\n",
       "      <th>signu_google</th>\n",
       "      <th>signu_weibo</th>\n",
       "      <th>...</th>\n",
       "      <th>action_detail_view_resolutions</th>\n",
       "      <th>action_detail_view_search_results</th>\n",
       "      <th>action_detail_view_security_checks</th>\n",
       "      <th>action_detail_view_user_real_names</th>\n",
       "      <th>action_detail_wishlist</th>\n",
       "      <th>action_detail_wishlist_content_update</th>\n",
       "      <th>action_detail_wishlist_note</th>\n",
       "      <th>action_detail_your_listings</th>\n",
       "      <th>action_detail_your_reservations</th>\n",
       "      <th>action_detail_your_trips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00023iyk9l</th>\n",
       "      <td>US</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001wyh0pz8</th>\n",
       "      <td>NDF</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0028jgx1x1</th>\n",
       "      <td>NDF</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>002qnbzfs5</th>\n",
       "      <td>US</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0035hobuyj</th>\n",
       "      <td>US</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 721 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           country_destination  age  gende_unknown  gende_male  gende_female  \\\n",
       "00023iyk9l                  US   31              1           0             0   \n",
       "001wyh0pz8                 NDF   -1              1           0             0   \n",
       "0028jgx1x1                 NDF   -1              1           0             0   \n",
       "002qnbzfs5                  US   26              0           0             1   \n",
       "0035hobuyj                  US   -1              0           0             1   \n",
       "\n",
       "            gende_other  signu_facebook  signu_basic  signu_google  \\\n",
       "00023iyk9l            0               0            1             0   \n",
       "001wyh0pz8            0               0            1             0   \n",
       "0028jgx1x1            0               0            1             0   \n",
       "002qnbzfs5            0               1            0             0   \n",
       "0035hobuyj            0               0            1             0   \n",
       "\n",
       "            signu_weibo            ...             \\\n",
       "00023iyk9l            0            ...              \n",
       "001wyh0pz8            0            ...              \n",
       "0028jgx1x1            0            ...              \n",
       "002qnbzfs5            0            ...              \n",
       "0035hobuyj            0            ...              \n",
       "\n",
       "            action_detail_view_resolutions  action_detail_view_search_results  \\\n",
       "00023iyk9l                               0                                  5   \n",
       "001wyh0pz8                               0                                 66   \n",
       "0028jgx1x1                               0                                  9   \n",
       "002qnbzfs5                               0                                125   \n",
       "0035hobuyj                               0                                200   \n",
       "\n",
       "            action_detail_view_security_checks  \\\n",
       "00023iyk9l                                   0   \n",
       "001wyh0pz8                                   0   \n",
       "0028jgx1x1                                   0   \n",
       "002qnbzfs5                                   0   \n",
       "0035hobuyj                                   0   \n",
       "\n",
       "            action_detail_view_user_real_names  action_detail_wishlist  \\\n",
       "00023iyk9l                                   0                       0   \n",
       "001wyh0pz8                                   0                       0   \n",
       "0028jgx1x1                                   0                       0   \n",
       "002qnbzfs5                                   0                       0   \n",
       "0035hobuyj                                   0                       0   \n",
       "\n",
       "            action_detail_wishlist_content_update  \\\n",
       "00023iyk9l                                      4   \n",
       "001wyh0pz8                                      0   \n",
       "0028jgx1x1                                      0   \n",
       "002qnbzfs5                                      0   \n",
       "0035hobuyj                                     26   \n",
       "\n",
       "            action_detail_wishlist_note  action_detail_your_listings  \\\n",
       "00023iyk9l                            0                            0   \n",
       "001wyh0pz8                            0                            0   \n",
       "0028jgx1x1                            0                            0   \n",
       "002qnbzfs5                            0                            0   \n",
       "0035hobuyj                            0                            0   \n",
       "\n",
       "            action_detail_your_reservations  action_detail_your_trips  \n",
       "00023iyk9l                                0                         2  \n",
       "001wyh0pz8                                0                         0  \n",
       "0028jgx1x1                                0                         0  \n",
       "002qnbzfs5                                0                         0  \n",
       "0035hobuyj                                0                         0  \n",
       "\n",
       "[5 rows x 721 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "id_train = df_train1.index.values\n",
    "labels = df_train1['country_destination']\n",
    "\n",
    "# Label encoding for the categorical data eg: ...NDF -> 7, US -> 10...\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "X = df_train1.drop('country_destination', axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73815, 720)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gende_unknown</th>\n",
       "      <th>gende_male</th>\n",
       "      <th>gende_female</th>\n",
       "      <th>gende_other</th>\n",
       "      <th>signu_facebook</th>\n",
       "      <th>signu_basic</th>\n",
       "      <th>signu_google</th>\n",
       "      <th>signu_weibo</th>\n",
       "      <th>signu_0</th>\n",
       "      <th>...</th>\n",
       "      <th>action_detail_view_resolutions</th>\n",
       "      <th>action_detail_view_search_results</th>\n",
       "      <th>action_detail_view_security_checks</th>\n",
       "      <th>action_detail_view_user_real_names</th>\n",
       "      <th>action_detail_wishlist</th>\n",
       "      <th>action_detail_wishlist_content_update</th>\n",
       "      <th>action_detail_wishlist_note</th>\n",
       "      <th>action_detail_your_listings</th>\n",
       "      <th>action_detail_your_reservations</th>\n",
       "      <th>action_detail_your_trips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00023iyk9l</th>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001wyh0pz8</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0028jgx1x1</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>002qnbzfs5</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0035hobuyj</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 720 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age  gende_unknown  gende_male  gende_female  gende_other  \\\n",
       "00023iyk9l   31              1           0             0            0   \n",
       "001wyh0pz8   -1              1           0             0            0   \n",
       "0028jgx1x1   -1              1           0             0            0   \n",
       "002qnbzfs5   26              0           0             1            0   \n",
       "0035hobuyj   -1              0           0             1            0   \n",
       "\n",
       "            signu_facebook  signu_basic  signu_google  signu_weibo  signu_0  \\\n",
       "00023iyk9l               0            1             0            0        1   \n",
       "001wyh0pz8               0            1             0            0        0   \n",
       "0028jgx1x1               0            1             0            0        1   \n",
       "002qnbzfs5               1            0             0            0        0   \n",
       "0035hobuyj               0            1             0            0        1   \n",
       "\n",
       "                      ...             action_detail_view_resolutions  \\\n",
       "00023iyk9l            ...                                          0   \n",
       "001wyh0pz8            ...                                          0   \n",
       "0028jgx1x1            ...                                          0   \n",
       "002qnbzfs5            ...                                          0   \n",
       "0035hobuyj            ...                                          0   \n",
       "\n",
       "            action_detail_view_search_results  \\\n",
       "00023iyk9l                                  5   \n",
       "001wyh0pz8                                 66   \n",
       "0028jgx1x1                                  9   \n",
       "002qnbzfs5                                125   \n",
       "0035hobuyj                                200   \n",
       "\n",
       "            action_detail_view_security_checks  \\\n",
       "00023iyk9l                                   0   \n",
       "001wyh0pz8                                   0   \n",
       "0028jgx1x1                                   0   \n",
       "002qnbzfs5                                   0   \n",
       "0035hobuyj                                   0   \n",
       "\n",
       "            action_detail_view_user_real_names  action_detail_wishlist  \\\n",
       "00023iyk9l                                   0                       0   \n",
       "001wyh0pz8                                   0                       0   \n",
       "0028jgx1x1                                   0                       0   \n",
       "002qnbzfs5                                   0                       0   \n",
       "0035hobuyj                                   0                       0   \n",
       "\n",
       "            action_detail_wishlist_content_update  \\\n",
       "00023iyk9l                                      4   \n",
       "001wyh0pz8                                      0   \n",
       "0028jgx1x1                                      0   \n",
       "002qnbzfs5                                      0   \n",
       "0035hobuyj                                     26   \n",
       "\n",
       "            action_detail_wishlist_note  action_detail_your_listings  \\\n",
       "00023iyk9l                            0                            0   \n",
       "001wyh0pz8                            0                            0   \n",
       "0028jgx1x1                            0                            0   \n",
       "002qnbzfs5                            0                            0   \n",
       "0035hobuyj                            0                            0   \n",
       "\n",
       "            action_detail_your_reservations  action_detail_your_trips  \n",
       "00023iyk9l                                0                         2  \n",
       "001wyh0pz8                                0                         0  \n",
       "0028jgx1x1                                0                         0  \n",
       "002qnbzfs5                                0                         0  \n",
       "0035hobuyj                                0                         0  \n",
       "\n",
       "[5 rows x 720 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPROACH 01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "## Spliting of training dataset into 70% training data and 30% testing data randomly\n",
    "features_train, features_test, labels_train, labels_test = \n",
    "cross_validation.train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.574125084669\n"
     ]
    }
   ],
   "source": [
    "## Decision Tree \n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(features_train, labels_train)\n",
    "prediction = clf.predict(features_test)\n",
    "## Computing accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print accuracy_score(prediction, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.598961390833\n"
     ]
    }
   ],
   "source": [
    "## Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(features_train, labels_train)\n",
    "prediction = clf.predict(features_test)\n",
    "## Computing accuracy\n",
    "print accuracy_score(prediction, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVM \n",
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel=\"rbf\") \n",
    "clf.fit(features_train, labels_train)\n",
    "prediction = clf.predict(features_test)\n",
    "## Computing accuracy\n",
    "print accuracy_score(prediction, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPROACH 02:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset creation\n",
    "#### For the sake of understanding the behaviour of learning models, the training dataset is split into -\n",
    "##### - training set: (X_trainA, y_trainA)\n",
    "##### - validation set: (X_validA, y_validA)\n",
    "##### - test set: (X_testA, y_testA)\n",
    "\n",
    "### Learning architecture\n",
    "#### First layer: I am using 6 classifiers from scikit-learn (Support_Vector_Machines, Logistic_Regression, Random_Forest, Gradient_Boosting, Extra_Trees_Classifier, K_Nearest_Neighbors). All classifiers are used with (almost) default parameters. At this level, many other classifiers can be used. All classifiers are applied twice:\n",
    "\n",
    "   ##### Classifiers are trained on (X_train, y_train) and used to predict the class probabilities of (X_valid).\n",
    "   ##### Classifiers are trained on (X = (X_train + X_valid), y = (y_train + y_valid)) and used to predict the class probabilities of (X_test)\n",
    "\n",
    "#### Second layer: The predictions from the previous layer on X_valid are concatenated and used to create a new training set (XV, y_valid). The predictions on X_test are concatenated to create a new test set (XT, y_test). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Importing the classifier libraries\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from xgboost.sklearn import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating dataset\n",
    "#### Parameters can be changed to explore different types of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:\n",
      "X_trainA: (44289, 349), X_validA: (14763, 349), X_testA: (14763, 349) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Spliting data into train and test sets.\n",
    "XA, X_testA, yA, y_testA = train_test_split(X, y, test_size=0.2, \n",
    "                                        random_state=random_state)\n",
    "    \n",
    "#Spliting train data into training and validation sets.\n",
    "X_trainA, X_validA, y_trainA, y_validA = train_test_split(XA, yA, test_size=0.25, \n",
    "                                                      random_state=random_state)\n",
    "\n",
    "print('Data shape:')\n",
    "print('X_trainA: %s, X_validA: %s, X_testA: %s \\n' %(X_trainA.shape, X_validA.shape, \n",
    "                                                     X_testA.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### First layer (individual classifiers)\n",
    "#### All classifiers are applied twice:\n",
    "##### - Training on (X_trainA, y_trainA) and predicting on (X_validA)\n",
    "##### - Training on (XA, yA) and predicting on (X_testA)\n",
    "##### You can add / remove classifiers or change parameter values to see the effect on final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of individual classifiers (1st layer) on X_testA\n",
      "------------------------------------------------------------\n",
      "KNN:       logloss  => 2.7331216"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "#Defining the classifiers\n",
    "clfs = {'LR'  : LogisticRegression(random_state=random_state), \n",
    "        'SVM' : SVC(probability=True, random_state=random_state), \n",
    "        'RF'  : RandomForestClassifier(n_estimators=100, n_jobs=-1, \n",
    "                                       random_state=random_state), \n",
    "        'GBM' : GradientBoostingClassifier(n_estimators=50, \n",
    "                                           random_state=random_state), \n",
    "        'ETC' : ExtraTreesClassifier(n_estimators=100, n_jobs=-1, \n",
    "                                     random_state=random_state),\n",
    "        'KNN' : KNeighborsClassifier(n_neighbors=30)}\n",
    "    \n",
    "#predictions on the validation and test sets\n",
    "p_valid = []\n",
    "p_test = []\n",
    "   \n",
    "print('Performance of individual classifiers (1st layer) on X_testA')   \n",
    "print('------------------------------------------------------------')\n",
    "   \n",
    "for nm, clf in clfs.items():\n",
    "    #First run. Training on (X_trainA, y_trainA) and predicting on X_validA.\n",
    "    clf.fit(X_trainA, y_trainA)\n",
    "    yv = clf.predict_proba(X_validA)\n",
    "    p_valid.append(yv)\n",
    "        \n",
    "    #Second run. Training on (XA, yA) and predicting on X_testA.\n",
    "    clf.fit(XA, yA)\n",
    "    yt = clf.predict_proba(X_testA)\n",
    "    p_test.append(yt)\n",
    "       \n",
    "    #Printing out the performance of the classifier\n",
    "    print('{:10s} {:2s} {:1.7f}'.format('%s: ' %(nm), 'logloss  =>', log_loss(y_testA, yt)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with sklearn LogisticRegression -\n",
    "##### Predictions on X_validA are used as training set (XV) and predictions on X_testA are used as test set (XT).\n",
    "#####  Setting the multi-class logloss as objective function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating the data for the 2nd layer.\n",
    "XV = np.hstack(p_valid)\n",
    "XT = np.hstack(p_test)\n",
    "\n",
    "#By default the best C parameter is obtained with a cross-validation approach, doing grid search with\n",
    "#10 values defined in a logarithmic scale between 1e-4 and 1e4.\n",
    "#Change parameters to see how they affect the final results.\n",
    "lr = LogisticRegressionCV(Cs=10, dual=False, fit_intercept=True, \n",
    "                          intercept_scaling=1.0, max_iter=25,\n",
    "                          multi_class='ovr', n_jobs=1, penalty='l2', \n",
    "                          random_state=random_state,\n",
    "                          solver='lbfgs', tol=0.0001)\n",
    "\n",
    "lr.fit(XV, y_validA)\n",
    "y_lr = lr.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('Log_Reg:', 'logloss  =>', log_loss(y_testA, y_lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gradient boosting\n",
    "xgb = XGBClassifier(max_depth=5, learning_rate=0.1,\n",
    "                    n_estimators=10000, objective='multi:softprob', \n",
    "                    seed=random_state)\n",
    "xgb.fit(XV, y_validA, early_stopping_rounds=15, verbose=False)\n",
    "xgb.n_estimators = xgb.best_iteration\n",
    "xgb.fit(XV, y_validA)\n",
    "y_gb = xgb.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('XGB_Reg:', 'logloss  =>', log_loss(y_testA, y_gb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW - Is there any parameters configuration for LogisticRegression that produces better results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPROACH 03:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1.4] GridSearchCV\n",
    "##### - Training data is ready in [1.3]\n",
    "##### - Now, use GridSearchCV to run the algorithm with a range of parameters, \n",
    "##### - Then select the model that has the highest cross validated score based on the chosen measure of a performance, in this case accuracy is considered (http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html), but there are a range of metrics(http://scikit-learn.org/stable/modules/model_evaluation.html) we could use based on our needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] n_estimators=5, learning_rate=0.1, max_depth=5 ..................\n",
      "[CV]  n_estimators=5, learning_rate=0.1, max_depth=5, score=0.683230 -  57.6s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Latha\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\sklearn.py:203: DeprecationWarning: The seed parameter is deprecated as of version .6.Please use random_state instead.seed is deprecated.\n",
      "  'seed is deprecated.', DeprecationWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks       | elapsed:   57.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] n_estimators=5, learning_rate=0.1, max_depth=5 ..................\n",
      "[CV]  n_estimators=5, learning_rate=0.1, max_depth=5, score=0.693898 -  53.8s\n",
      "[CV] n_estimators=5, learning_rate=0.1, max_depth=5 .................."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Latha\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\sklearn.py:203: DeprecationWarning: The seed parameter is deprecated as of version .6.Please use random_state instead.seed is deprecated.\n",
      "  'seed is deprecated.', DeprecationWarning)\n",
      "C:\\Users\\Latha\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\sklearn.py:203: DeprecationWarning: The seed parameter is deprecated as of version .6.Please use random_state instead.seed is deprecated.\n",
      "  'seed is deprecated.', DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV]  n_estimators=5, learning_rate=0.1, max_depth=5, score=0.689257 -  54.0s\n",
      "Best score: 0.689\n",
      "Best parameters set:\n",
      "\tlearning_rate: 0.1\n",
      "\tmax_depth: 5\n",
      "\tn_estimators: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  2.8min finished\n",
      "C:\\Users\\Latha\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\sklearn.py:203: DeprecationWarning: The seed parameter is deprecated as of version .6.Please use random_state instead.seed is deprecated.\n",
      "  'seed is deprecated.', DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import decomposition, grid_search\n",
    "\n",
    "# Grid Search - Used to find best combination of parameters\n",
    "XGB_model = xgb.XGBClassifier(objective='multi:softprob', \n",
    "                              subsample=0.5, colsample_bytree=0.5, \n",
    "                              seed=0)\n",
    "param_grid = {'max_depth': [5], 'learning_rate': [0.1], \n",
    "              'n_estimators': [5]}\n",
    "#param_grid = {'max_depth': [3, 4, 5], 'learning_rate': [0.1, 0.3], 'n_estimators': [25, 50]} ##Note running this step can take a significant amount of time, might take hours as well.\n",
    "model = grid_search.GridSearchCV(estimator=XGB_model, param_grid=param_grid,\n",
    "                                 scoring='accuracy', verbose=10, n_jobs=1, \n",
    "                                 iid=True, refit=True, cv=3)\n",
    "\n",
    "#model.fit(X, y)\n",
    "model.fit(features_train, labels_train)\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB_Reg:             logloss  => 1.6222701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Latha\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\sklearn.py:203: DeprecationWarning: The seed parameter is deprecated as of version .6.Please use random_state instead.seed is deprecated.\n",
      "  'seed is deprecated.', DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "#Gradient boosting\n",
    "#xgb = XGBClassifier(max_depth=5, learning_rate=0.1,\n",
    "#                    n_estimators=5, objective='multi:softprob', \n",
    "#                    seed=0)\n",
    "#xgb.fit(features_train, labels_train, verbose=False)\n",
    "y_gb = model.predict_proba(features_test)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('XGB_Reg:', 'logloss  =>', \n",
    "                                    log_loss(labels_test, y_gb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBClassifier Parameters -\n",
    "\n",
    "##### objective='multi:softprob' (http://xgboost.readthedocs.io/en/latest/parameter.html) --> Specify the learning task and the corresponding learning objective\n",
    "\n",
    "##### subsample=0.5 [default=1] --> subsample ratio of the training instance. Setting it to 0.5 means that XGBoost randomly collected half of the data instances to grow trees and this will prevent overfitting.\n",
    "\n",
    "##### colsample_bytree=0.5 [default=1] --> subsample ratio of columns when constructing each tree.\n",
    "\n",
    "##### max_depth [default=6] --> maximum depth of a tree, increase this value will make the model more complex / likely to be overfitting. \n",
    "\n",
    "##### n_estimators --> Number of boosted trees to fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.5] Making the Predictions\n",
    "\n",
    "- Now that we have trained the model based on the best parameters,\n",
    "- Next step is to use the model to make predictions for the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "# Make predictions\n",
    "y_pred = model.predict(features_test)\n",
    "y_pred_prob = model.predict_proba(features_test) ##select the 5 best predictions\n",
    "\n",
    "#Print model report:\n",
    "print \"\\nModel Report\"\n",
    "print \"Accuracy : %.4g\" % accuracy_score(labels_test, y_pred)\n",
    "#print \"AUC Score (Train): %f\" % roc_auc_score(labels_test, y_pred_prob)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22145L,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1.5.1] Extracting the testing data out of the combined dataset (df_all) we created for the cleaning and transformation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare test data for prediction\n",
    "df_test1.set_index('id', inplace=True)\n",
    "df_test1 = pd.merge(df_test1.loc[:,['date_first_booking']], df_all1, how='left', left_index=True, right_index=True, sort=False)\n",
    "X_test1 = df_test1.drop('date_first_booking', axis=1, inplace=False)\n",
    "X_test1 = X_test1.fillna(-1)\n",
    "id_test = df_test1.index.values\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict_proba(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62096, 721)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'date_first_booking', u'age', u'gende_unknown', u'gende_male',\n",
       "       u'gende_female', u'gende_other', u'signu_facebook', u'signu_basic',\n",
       "       u'signu_google', u'signu_weibo',\n",
       "       ...\n",
       "       u'action_detail_view_resolutions', u'action_detail_view_search_results',\n",
       "       u'action_detail_view_security_checks',\n",
       "       u'action_detail_view_user_real_names', u'action_detail_wishlist',\n",
       "       u'action_detail_wishlist_content_update',\n",
       "       u'action_detail_wishlist_note', u'action_detail_your_listings',\n",
       "       u'action_detail_your_reservations', u'action_detail_your_trips'],\n",
       "      dtype='object', length=721)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62096, 720)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reference: Parameter Tuning in XGBoost\n",
    "   #### https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
